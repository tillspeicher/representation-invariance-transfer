# Transforms vs Other Factors

This experiment investigates how important training a model on data with the same transformations as the target task (i.e. to have the same invariances) is compared to other pre-training factors for transfer learning.
To do this, two sets of models are trained using the Transforms2D dataset.
In one case the training data is generated with the same transformations as the target/test dataset, and in the second case a disjoint set of transformations is used.
For example, if in the target task samples are generated by transforming objects by translation, color jittering and blurring, then one set of models is trained with these transformations, whereas the other set might use for example rotations, contrast changes and noise artifacts.

Both sets of models are trained by varying some other potentially important pre-trianing parameter, such as the number of training examples, the model architecture or the relationship of the classes in the training dataset with those in the target dataset (subset, same, disjoint, superset).

## Usage

To run the experiment, use the following command:

```bash
poetry run python src/main.py +tvo=<config_name>
```

where `config_name` can be e.g. `test` or `t2d_v-class-rel_ft-lp`.
See `configs.py` for all available configurations.
